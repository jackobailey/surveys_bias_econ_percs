---
title: |
  | Political Surveys Bias Self-Reported
  | Economic Perceptions\thanks{Thank you to Ed Fieldhouse, Jon Mellon, Marta Cantijoch, and Rosie Shorrocks for reading and commenting on early versions of this paper and to Adam McDonnell and Jemma Connor at YouGov for all their help in fielding my experiment.}
author: |
  | Jack Bailey\thanks{Research Associate, Department of Politics, The University of Manchester, UK. If you have any comments or questions, feel free to contact me either by email (\href{mailto:jack.bailey@manchester.ac.uk}{jack.bailey@manchester.ac.uk}) or on Twitter (\href{https://www.twitter.com/PoliSciJack}{@PoliSciJack}).}
abstract: |
  | If voters are to hold governments to account for the state of the economy, they must know how it has changed. Indeed, this is a prerequisite for democratic accountability. Yet the perceptions that voters report often show signs of clear partisan bias. At present, we do not know if this bias is real or instead due to priming in political surveys. To test this, I assign subjects at random to either a political or non-political survey. I then record their economic perceptions and compare the results for each group. I show that political surveys do worsen partisan bias, though only among supporters of the incumbent party. Still, much partisan bias remains unexplained, even in the non-political condition. So, while economic perception items remain biased, we can at least be sure that most people respond to them in a similar way no matter the survey context.
indent: yes
fontsize: 12pt
geometry: margin = 1.15in
subparagraph: yes
compact-title: false
linkcolor: black
urlcolor: violet
citecolor: black
bibliography: _assets/master.bib
biblio-style: _assets/apsr.bst
classoption: a4paper
output: 
  bookdown::pdf_document2: 
    latex_engine: xelatex
    toc: false
    keep_tex: false
    includes:
      in_header:
        - _assets/rmd-preamble.tex
    number_sections: false
    highlight: kate
    fig_caption: true
---
<!-- Latex setup -->

\doublespacing

\thispagestyle{empty}
\clearpage

\pagebreak

\setcounter{page}{1}

```{r setup, include = F}

# Load packages

library(kableExtra)
library(tidyverse)
library(magrittr)
library(jbmisc) # https://github.com/jackobailey/jbmisc
library(brms)
library(here)


# Load fitted models

m1 <- readRDS(here("_output", "m1.rds"))
m2 <- readRDS(here("_output", "m2.rds"))
m3 <- readRDS(here("_output", "m3.rds"))
m4 <- readRDS(here("_output", "m4.rds"))


# Load posterior treatment effects

cates_m1 <- readRDS(here("_output", "cates_m1.rds"))
cates_m2 <- readRDS(here("_output", "cates_m2.rds"))
cates_m3 <- readRDS(here("_output", "cates_m3.rds"))
cates_m4 <- readRDS(here("_output", "cates_m4.rds"))


# Get posterior shift effects for m1

m1_posts <- 
  posterior_samples(
    m1,
    pars = c("b_tTreatment", "b_pInc", "b_pOpp")
  ) %>% 
  rename(
    treatment_inc = "b_tTreatment:pInc",
    treatment_opp = "b_tTreatment:pOpp"
  )


# Load figure scripts

source(here("_scripts", "005_ordinal_fig.R"))
source(here("_scripts", "006_national_fig.R"))
source(here("_scripts", "007_personal_fig.R"))
source(here("_scripts", "008_eu_fig.R"))
source(here("_scripts", "009_multi_fig.R"))
load(here("_output", "power_fig.rda"))
source(here("_scripts", "011_prior_figs.R"))
source(here("_scripts", "012_raw_plot.R"))
source(here("_scripts", "013_bias_plot.R"))


# Tell knitr to use Cairo PDF when rendering plots so that it uses nice fonts

knitr::opts_chunk$set(dev = "cairo_pdf")

```


# Introduction

To hold governments to account for the state of the economy, voters must first know how it has changed. Indeed, this is an essential prerequisite for democratic accountability [@healy2013; @ashworth2012]. Thus, voters should notice that conditions improve when the economy grows and worsen when it shrinks. Just as this variation in perception is important, so too are its consequences. If voters are to reward and punish appropriately, then they should be more likely to support the incumbent where they also think that the economy has improved. This two-step process --- first, of economic updating; second, of electoral sanctioning --- is crucial for good governance. Rather than force voters to suffer fools, it lets them "kick the rascals out" if they fail to live up to expectations [@stegmaier2019].

Though this idea has great normative appeal, reality often falls short. This is because voters are not so dispassionate when it comes to judging economic management. Instead, all manner of considerations influence the decisions they make. For instance, voters tend to rely on pre-existing beliefs when processing new information. Evidence of this behavior is rife in political surveys, where respondents often report economic perceptions that show clear signs of partisan bias: those who support the incumbent tend to be more positive, and those who support the opposition more negative, than those who support no party at all [for recent evidence of this phenomenon, see @bailey2019; @bisgaard2019; @devries2018].

Given the potential ramifications, much work now focuses on mitigating this bias. Yet we still do not know if it is meaningful or, instead, a result of partisan priming in political surveys. In test this possibility in this short research note. To do so, I use new survey experimental data from the 2019 UK General Election campaign.

I find that political surveys worsen partisan bias in voters' self-reported economic perceptions. But this is true only for those who voted for the incumbent at the last election. What's more, much partisan bias remains unexplained. Thus, while economic perception items are far from perfect, survey researchers and economic voting scholars can at least be sure that most people respond to them in a similar way no matter the survey context.


# Economic Perceptions, Partisan Bias, and Political Surveys

Party identification biases the economic perceptions that voters report [see, for example, @devries2018; @bartels2002; @conover1987]. What's more, this bias serves to undermine accountability mechanisms that make democracy possible [@healy2013; @anderson2007]. The reason for this is simple. Partisan voters report economic perceptions that paint their party in a positive light. For example, incumbent supporters tend to report more positive economic perceptions. Opposition supporters, instead, tend to report more negative ones. Thus, we cannot be sure that voters will hold their party to account on the basis of economic management when it gains power.

While we know that party identification affects reported economic perceptions, we are less certain why this is the case. Clearly, this is an important gap in our understanding. If we do not know what causes partisan bias, we cannot hope to mitigate its worst effects. To this end, the literature on economic perceptions offers four competing hypotheses. Yet, though they are competing, they are not exclusive. Rather, all four likely influence the economic perceptions that voters report to some extent or another. I discuss each below in turn.

The first potential cause of partisan bias is consistency-motivated reasoning [@kunda1990]. This holds that voters weight new information based on how congruent it is with their existing beliefs [@hill2017]. This behavior weakens voters' ability to hold parties to account. Still, it is easy to see why it might play a role in their economic calculus. Existing research shows that the economy shapes wider perceptions of party competence [@green2017]. And this can lead to much psychological discomfort for partisan voters [@groenendyk2013]. Consider an incumbent supporter during the middle of an economic downturn. Such a voter must contend with two conflicting beliefs: that the economy is doing badly and that their own party is managing the economy well. Consistency-motivated reasoning offers them a way out of this predicament. By down-weighting incongruent information, they can either ignore evidence that the economy is doing badly or, instead, update their perceptions based only on information that pins the blame on someone else [@bisgaard2019; @bisgaard2015; @tilley2011].

The second potential cause is partisan cuing [@brady1985]. Like consistency-motivated reasoning, this too is psychological in nature. It suggests that voters make use of cognitive shortcuts. This is necessary because, as have long known, many voters pay little attention to politics [@campbell1960]. As such, they have a hard time when it comes to making decisions about political matters. The partisan cuing literature argues that they resolve this problem by making a simple substitution. Rather than derive their own belief, they rely on their favorite party's position on the issue instead. They may do this either out of party loyalty or the belief that they would have come to the same conclusion were they fully-informed [@ramirez2014; @brader2013a]. Evidence in favor of partisan cuing is most striking where it concerns party elites. @bisgaard2018, for instance, shows that when the Danish government began to consider the budget deficit in a negative light, its supporters came to do so too despite not having done so just a short time before.

The third potential cause is expressive responding [@schaffner2018; @bullock2015]. Unlike the preceding two explanations, it does not rely on voter psychology to explain partisan bias. Instead, it contends that survey respondents use survey items to signal their support for a particular party. For example, a respondent might report that the economy has gotten better not because they believe it to be so, but because they support the incumbent party. Recent survey experimental evidence shows that expressive responding almost certainly occurs. Though concerned with factual questions, @bullock2015 and @prior2015 run similar experiments where they manipulate the incentive to engage in expressive responding. Respondents in the treatment groups received a small cash reward where they admitted either to not knowing the answer or happened to give the correct answer to a series of factual questions about the economy and other policy-related topics. Respondents in the control groups received no such reward. In both cases, the authors find that partisan disagreement was lower under the treatment than under the control, implying that some responses serve only to signal respondents' party preferences.

The fourth and final potential cause is item-order effects. These occur where the order in which survey questions are asked affects the answers that respondents give. If non-political items precede political ones, they may *personalize* respondents' answers. Likewise, where political items precede non-political ones, they may *politicize* them instead [@sears1983]. Such item-order effects can be both large and long-lasting. Indeed, even where several buffer items separate them, political questions still come to bias the economic perceptions that respondents report [@wilcox1993]. Further, as many electoral surveys begin by asking their respondents how they voted or how they intend to vote, this politicization of economic perception items is probably relatively common.

Though distinct, all four causes share a common catalyst: the political survey context. That is to say, partisan priming in political surveys might worsen their effect. For the sake of illustration, consider expressive responding. If the survey context implies that the survey administrator does not care about politics, then respondents also face fewer incentives to engage in partisan cheer-leading. Likewise, consider motivated reasoning and partisan cuing. If the survey context does not encourage respondents to consider the economy through a partisan lens, then it seems reasonable to expect them to be less likely to rely on partisanship to determine what they think about the state of the economy. It is for this reason that consumer most confidence surveys rarely ask about respondents' party affiliation [@curtin2019].

As a result, the political survey context *itself* might moderate how party identification affects the economic perceptions that voters report. And, given that partisan bias varies direction based on party identification, so too should political survey effects. Thus, we should expect incumbent supporters to be *more* likely to report positive and *less* likely to report negative economic perceptions in political compared to non-political surveys. We should expect opposition supporters, instead, to do the opposite. This implies the two following hypotheses:

\vspace{.5cm}
\setlength\parindent{0em}

\textsf{\textbf{Hypothesis 1:}} Incumbent partisans will be more likely to choose *positive* answers to economic perception items in a political survey than similar incumbent partisans in a non-political survey.

\vspace{.5cm}

\textsf{\textbf{Hypothesis 2:}} Opposition partisans will be more likely to choose *negative* answers to economic perception items in a political survey than similar opposition partisans in a non-political survey.

\vspace{.5cm}
\setlength\parindent{2em}


# Experimental Design

I use a simple survey experiment to test my hypotheses. The market research and polling company YouGov collected the corresponding data from its panel of eligible British voters^[Note that while YouGov uses non-probability sampling, its data are not from a convenience sample. Rather the company ensures that its data are nationally-representative using a method it calls "active sampling" [@twyman2008]. This approach has proven robust and the company's surveys often yield results substantively similar to those collected using random probability sampling [@sanders2007].]. Further, data collection occurred between the 6\textsuperscript{th} and the 8\textsuperscript{th} November 2019. This was a deliberate choice and ensured that data collection coincided with the beginning of the campaign for the 2019 UK General Election.

The British case is especially useful and provides a strong test of my argument for two reasons. First, the data collection process coincided with the start of the 2019 UK General Election campaign. This implies that my subjects were exposed to a general politicization of the information environment. As a result, we might expect participants to bias their responses in *non-political* surveys too. Thus, any differences between my treatment group and my control group will likely be conservative. Second, data collection occurred at a time of economic uncertainty. Though the economy was not in recession, it was not growing much either. The most recent GDP data at the time showed that the UK economy had contracted by 0.2% in the previous quarter. This is important as new evidence shows that even strong partisans "get it" when the going gets tough [@bisgaard2019; @devries2018] and that this leads partisan bias to diminish [@bailey2019; @stanig2013]. As such, it seems reasonable to expect the economic circumstances at the time to provide less partisan bias for my treatment to manipulate.

In the first stage of the experiment, I drew a blocked sample from YouGov's online panel^[This design is deliberately non-representative as it ensures that I am just as able to detect a treatment effect for incumbent voters as I am for opposition voters or non-voters. As such, I do not weight my data. Regardless, this is likely to make little difference. As @miratrix2018 show, also using YouGov data, for "high-quality, broadly representative samples recruited by top online survey firms, sample quantities, which do not rely on weights, are often sufficient" (p. 275).]. The first blocked contained only those panelists who had voted for the incumbent Conservative Party at the last election in 2017, the second only those who had voted for an opposition party, and the third only those who had not voted at all^[Retention was high. Just 3.5% (91) of respondents failed to finish the survey. Of these, 48 left before being assigned to a condition, 18 left after being assigned to the treatment, and 25 left after being assigned to the control.]. To determine my sample size, I conducted a simulation-based power analysis. The results from 6,000 simulated experiments showed that I would need a sample of around 2,500 respondents to reach a power level of 80%^[For more information, see appendix A.].

I do not have my participants report their voting behavior during the experiment, but rely instead on contemporaneous data that YouGov collected after the 2017 election. As such, misreporting bias or other related issues should be low. Some might argue that it would be better to use participants' *current* party identification and not how they voted in the past. After all, attitudes and choices change over time. While this is a reasonable objection, it is not possible to include such an item without undermining the non-political survey context. Further, using past voting behavior has one particular advantage: voters cannot undo it. This may explain why it appears to exert such a considerable effect on the economic perceptions that voters report in political surveys [@anderson2004].

In the second stage of the experiment, I exploited YouGov's day-to-day operations to administer my treatment. As a large commercial polling company, YouGov runs many simultaneous political and non-political surveys. It also runs them in tandem. As a result, panelists are used to surveys that concern one topic then switch to another. My treatment group first completed a version of YouGov's standard voting intention poll. This includes five questions that concern voting behavior and the perception of party leaders. The control group, instead, completed a survey on dental hygiene. This had an almost identical structure to the political survey. For example, it asked the same number of questions, the same type of questions, and included the same number of response options in all cases. Further, it also used only questions that YouGov had fielded in the past to ensure that it was believable^[A full questionnaire is available in appendix B.]. In all cases, participants had an equal chance of being assigned to the treatment or to the control.

In the third and final stage of the experiment, I again exploited YouGov's day-to-day operations, this time to measure my participants' economic perceptions. After receiving their treatment, both groups saw the topic of the survey switch from politics or dental hygiene to the economy. I then asked them to report their own retrospective economic and financial perceptions. As I used a sample of eligible British voters, I followed the lead of the British Election Study Internet Panel [@fieldhouse2020a] and had my participants answer the two following questions:

- Now, a few questions about economic conditions. How does the *financial situation of your household* now compare with what it was 12 months ago?

- How do you think the *general economic situation in this country* has changed over the *last 12 months*?

These items have their origins in consumer confidence surveys [@katona1951], entered political science via *The American Voter* [@campbell1960], and are now ubiquitous in economic voting research [@lewis-beck2007]^[Note that the consumer confidence surveys from which these items originate rarely field questions of partisanship as they are known to engender emotional states that bias how survey respondents answer economic perception questions [@curtin2019]]. By and large, the literature on economic perceptions and partisan bias focuses only on national-level items [for recent examples, see @dassonneville2019; @anson2017; @hansford2015]. While I do include this item, I also asked my subjects to report their personal financial perceptions too. Doing so serves a useful purpose: it provides both a benchmark for any national-level effects and helps to prevent an unusual one-question-long topic.

In both cases, my subjects faced the exact same response options. They could answer each question on a five-point Likert-type scale that ranged from *"1 -- Got a lot worse"* to *"5 -- Got a lot better"*. They could also report that they did not know how either the national or their own personal economic situation compared to what it was 12 months ago. Where this was the case, I removed these participants using list-wise deletion^[List-wise deletion can produce biased estimates if data are not missing completely at random. As I estimate treatment effects on each specific response category, this should not be an issue. Still, simulation studies show that list-wise deletion yields less biased estimates than multiple imputation where data are missing not at random [@pepinsky2018]. Even so, I include these data as a robustness check (see appendix D). This does not change my results. Further, participants were no more likely to answer "Don't know" under the treatment than the control.].

```{r raw-plot, fig.cap = "Descriptive figures show that incumbent partisans (left column) tend to be more positive than nonvoters (right column) under both the treatment and control. Likewise, opposition partisans (middle column) tend to be more negative than nonvoters. Further, these figures also suggest evidence in favour of my first hypothesis that incumbent partisans in the treatment would be more positive than incumbent partisans in the control, though not my second hypothesis that opposition partisans in the treatment would be more negative than opposition partisans in the control.", fig.width = 6, fig.height = 3.25, echo = F}
raw_fig
```

Figure \@ref(fig:raw-plot) shows the raw percentages for each response option stratified by party and treatment status. As we would expect, these show that incumbent partisans are more positive than do nonvoters. Further, this is true under both the treatment and the control. For example, `r raw_dta %>% filter(p == "Incumbent (Treatment)", resp %in% c("Little better", "Lot better")) %>% pluck(4) %>% sum() %>% scales::percent(accuracy = .1)` of incumbent partisans in the treatment condition (a political survey) said that the economy had gotten a lot or a little better while only `r raw_dta %>% filter(p == "Nonvoter (Treatment)", resp %in% c("Little better", "Lot better")) %>% pluck(4) %>% sum() %>% scales::percent(accuracy = .1)` of nonvoters in the treatment condition said the same. Likewise, opposition partisans are more negative than nonvoters. Among opposition partisans in the treatment condition, `r raw_dta %>% filter(p == "Opposition (Treatment)", resp %in% c("Little worse", "Lot worse")) %>% pluck(4) %>% sum() %>% scales::percent(accuracy = .1)` said that the economy had got a lot or a little worse whereas `r raw_dta %>% filter(p == "Nonvoter (Treatment)", resp %in% c("Little worse", "Lot worse")) %>% pluck(4) %>% sum() %>% scales::percent(accuracy = .1)` of similar nonvoters made the same judgment.

Though still descriptive, figure \@ref(fig:raw-plot) also suggests evidence in favor of my first hypothesis. Incumbent partisans in the political survey treatment condition were `r (sum(raw_dta$pct[raw_dta$p == "Incumbent (Treatment)" & raw_dta$resp %in% c("Lot better", "Little better")]) - sum(raw_dta$pct[raw_dta$p == "Incumbent (Control)" & raw_dta$resp %in% c("Lot better", "Little better")])) %>% scales::percent(accuracy = .1)` more likely to say that things had gotten better than similar incumbent partisans in the control condition. They were also `r (sum(raw_dta$pct[raw_dta$p == "Incumbent (Treatment)" & raw_dta$resp %in% c("Lot worse", "Little worse")]) - sum(raw_dta$pct[raw_dta$p == "Incumbent (Control)" & raw_dta$resp %in% c("Lot worse", "Little worse")])) %>% scales::percent(accuracy = .1)` less likely to say that the economy had gotten worse. Yet these data suggest little evidence in favor of my second hypothesis. Indeed, opposition partisans in the political survey treatment condition were `r (sum(raw_dta$pct[raw_dta$p == "Opposition (Treatment)" & raw_dta$resp %in% c("Lot better", "Little better")]) - sum(raw_dta$pct[raw_dta$p == "Opposition (Control)" & raw_dta$resp %in% c("Lot better", "Little better")])) %>% scales::percent(accuracy = .1)` more likely to say that things had gotten better and `r (sum(raw_dta$pct[raw_dta$p == "Opposition (Treatment)" & raw_dta$resp %in% c("Lot worse", "Little worse")]) - sum(raw_dta$pct[raw_dta$p == "Opposition (Control)" & raw_dta$resp %in% c("Lot worse", "Little worse")])) %>% abs() %>% scales::percent(accuracy = .1)` less likely to say that things had gotten worse than opposition partisans in the non-political control condition. While informative, any inferences that we make from these descriptive statistics do not account for the uncertainty inherent in the sample. To do so requires a more rigorous approach, which I describe in greater detail below.


# Estimating Response-Specific Treatment Effects

Economic perception items yield ordinal data. Yet many researchers treat them as continuous. This is convenient, as it allows them to estimate treatment effects using only a simple comparison of means. But this simplicity belies drawbacks that include false positives, false negatives, and even estimates with incorrect signs [@liddell2018].

One argument for treating these items as continuous is that while the outcome is ordinal, subgroup means and differences are continuous. This is true. But it is not clear what such treatment effects even imply. Indeed, when ordinal variables have three or more response options, there are an *infinite* combination of response distributions that could produce any given difference in means.

Better then to estimate treatment effects for the choices that survey respondents really face: the ordinal variable's response options. To do so, one might expect to use ordered regression. But these models face similar problems. Figure \@ref(fig:ord-plot) shows why. Ordered regression treats the ordinal distribution that we observe (bottom row) as a function of a continuous one that we do not (top row). It then uses a set of threshold parameters (gray dotted lines) to convert between the two. These divide the latent continuous distribution into as many segments as there are response options. The area between two thresholds then gives the probability of each response occurring.

The first column shows the baseline case. Here, each response has an equal probability. To change this, we can adjust either the latent distribution's mean or variance. This has three consequences. When we adjust the mean, the latent distribution *shifts* up or down the scale (second column). This alters the area between the thresholds and moves the ordinal distribution in the same direction. When we instead adjust the variance, the latent distribution either *compresses*, squeezing the ordinal distribution's probability mass (third column), or *disperses*, piling up probability mass at the extremes (fourth panel).

```{r ord-plot, fig.cap = "Ordered regression assumes that the observed ordinal scale is a function of a latent continuous one. Imagine that each column shows a different treatment group and that the baseline group is the control. Participants have an equal probability of selecting each response. When the the latent scale *shifts*, so too does the probability of selecting a higher value on the observed scale. Likewise, when it *compresses* or *disperses*, the observed scale follows suit. As we can see, each may have a large effect. Yet most ordered regression models account only for shift.", fig.width = 6, fig.height = 3.25, echo = F}
ord_fig
```

As figure \@ref(fig:ord-plot) shows, compression and dispersion can have large effects on the ordinal distribution. Yet conventional ordered regression accounts only for shift. This is a problem, as treatments may affect the outcome without shifting the probability mass to one end or the other. Dealing with this is difficult using Frequentist methods. Thus, in line with recent recommendations [@liddell2018], I use Bayesian methods instead^[Note that Bayesian models require prior distributions. In this case, I specify a set of conservative and weakly informative priors for each parameter. I discuss my choices in greater detail in appendix C.].

My model is as follows. Let $E_{i}$ be person $i$'s reported retrospective economic perceptions. In line with existing economic voting research, this item is measured on a five-point Likert-type scale as described above and which takes a value that varies between 1 $=$ "Got a lot worse" and 5 $=$ "Got a lot better". In order to model the data as ordinal, we assume that the observed ordered variable, $E_{i}$, is a function of some latent continuous variable, $E_{i}^{*}$. We then assume that this latent continuous variable follows a normal distribution with mean, $\mu_{i}$, and standard deviation, $\sigma_{i}$:

\begin{equation*}
E_{i}^{*} \sim \text{Normal}(\mu_{i}, \sigma_{i})
\end{equation*}

Likewise, the observed ordinal outcome variable, $E_{i}$, takes a particular value as follows:

\begin{equation*}
E_{i} = k \text{ if } \tau_{k-1} \leq E_{i}^{*} \leq \tau_{k} \text{ for } k = 1, ..., K
\end{equation*}

Here, $\tau_{k}$ for $k = 0, ..., K$ represent threshold parameters which segment the latent continuous distribution. We fix the $0^{th}$ and $k^{th}$ thresholds equal to $- \infty$ and $+ \infty$, such that $- \infty = \tau_{0} < \tau_{1} < ... < \tau_{k-1} < \tau_{k} = \infty$. As such, the probability that $E_{i} = k$ is:

\begin{equation*}
\text{Pr}(E_{i} = k) = \Phi(\frac{\tau_{k} - \mu_{i}}{\sigma_{i}}) - \Phi(\frac{\tau_{k-1} - \mu_{i}}{\sigma_{i}})
\end{equation*}

Where $\Phi$ is the cumulative distribution function of the normal distribution with mean $\mu_{i}$ and standard deviation $\sigma_{i}$. As I discuss above, both influence the ordinal distribution that we observe. Likewise, both may also vary according either to party preference or treatment status:

\begin{align*}
\mu_{i} &= \beta_{1} T_{i} + \beta_{2} I_{i} + \beta_{3} O_{i} + \beta_{4} (T_{i} \times I_{i}) + \beta_{5} (T_{i} \times O_{i}) \\
log(\sfrac{1}{\sigma_{i}}) &= \delta_{1} T_{i} + \delta_{2} I_{i} + \delta_{3} O_{i} + \delta_{4} (T_{i} \times I_{i}) + \delta_{5} (T_{i} \times O_{i})
\end{align*}

Here, $T_{i}$ takes the value $1$ where person $i$ is in the treatment group. Likewise, $I_{i}$ and $O_{i}$ take the value $1$ where person $i$ voted for the incumbent or an opposition party at the last election, respectively. Rather than model $\sigma_{i}$, I instead model $log(\sfrac{1}{\sigma_{i}})$. Further, to identify the model, I fix $\sigma_{i}$ to 1 for the baseline category (non-voters).

Though somewhat complex, this method is robust to the problems I discuss above. Still, like any ordered regression model, it produces parameters that are hard to interpret. Fortunately, as Bayesian models are generative [@lambert2018] we can have them estimate treatment effects on the more intuitive probability scale while also incorporating any inherent uncertainty. I do this below, and compute my treatment effects for each response category as follows^[As it is much easier to interpret treatment effects measured in percentage point terms than as log-odds, and because space is limited, I save any regression tables for the appendix (see tables \@ref(tab:tabA1)--\@ref(tab:tabA4)).]:

\begin{equation*}
\text{ATE}_{k} = \text{Pr}(E_{i} = k | T_{i} = 1) - \text{Pr}(E_{i} = k \mid T_{i} = 0)
\end{equation*}


# Results

```{r nat-plot, fig.cap = 'Political surveys cause incumbent voters to report different economic perceptions (left panel). To this end, they were less likely to say that the economy had either "got a little worse" or "got a lot worse". They were also more likely to say that the economy had "stayed the same" or that it had "got a little better". Note that the treatment had little effect on reporting that the economy had "got a lot better" for any group and also that these estimates were very precise. This is because almost no one said that the economy had "got a lot better" in either group. Here, density plots show the posterior distribution of conditional average treatment effects. Further, black bars show their 95% credible intervals and point estimates their medians.', fig.width = 6, fig.height = 3.25, echo = F}
nat_fig
```

In line with my expectations, my results show that political surveys *do* affect the economic perceptions that respondents report in political surveys. Yet, these political survey treatment effects are limited only to those respondents who voted for the incumbent Conservative Party at the last general election in 2017^[This finding is robust to a range of tests. See appendix D.]. Figure \@ref(fig:nat-plot) shows these estimates in graphical form. Here, density plots show the posterior distribution of each treatment effect, black bars their 95% credible intervals, and point estimates their median. Each density curve reflects the difference in the probability of reporting a given response under the treatment versus the control. Thus, a positive value implies that the political survey treatment increased the probability of a respondent picking a given response by a given number of percentage points compared to similar respondents in the non-political survey control.

The left-most panel shows how the treatment affected those who voted for the incumbent party in 2017. In general, these effects are in line with my expectation that incumbent-supporting subjects would be more positive in the political survey. In the treatment group, these respondents were `r (cates_m1$cate[cates_m1$p == "Incumbent" & cates_m1$resp == "Lot Worse"] * 100) %>% abs() %>% in_text(text = "percentage points", inside = F)` less likely to say that the economy "got a lot worse" over the past twelve months and `r (cates_m1$cate[cates_m1$p == "Incumbent" & cates_m1$resp == "Little Worse"] * 100) %>% abs() %>% in_text(text = "percentage points", inside = F)` less likely to say that it "got a little worse". In comparison, they were `r (cates_m1$cate[cates_m1$p == "Incumbent" & cates_m1$resp == "Little Better"] * 100) %>% in_text(text = "percentage points", inside = F)` *more* likely to say that the economy "got a little better".

Interestingly, these subjects were no more likely to say that the economy "got a lot better" `r (cates_m1$cate[cates_m1$p == "Incumbent" & cates_m1$resp == "Lot Better"] * 100) %>% in_text(inside = T)`. This effect was also much more precise than for other responses. Though this may seem unusual, it arises only because almost no one reported that the economy "got a lot better". This is not uncommon, at least in the British case, even when the economy is booming [see @bailey2019]. Finally, those reporting that the economy "stayed the same" made up the difference. These participants were `r (cates_m1$cate[cates_m1$p == "Incumbent" & cates_m1$resp == "Stayed Same"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to pick this option under the treatment compared to the control.

Contrary to my expectations, the effect of taking a political survey was less clear where participants voted for an opposition party at the last election. These subjects were not much more likely to say that the economy "got a lot better" `r (cates_m1$cate[cates_m1$p == "Opposition" & cates_m1$resp == "Lot Better"] * 100) %>% in_text()`, "got a little better" `r (cates_m1$cate[cates_m1$p == "Opposition" & cates_m1$resp == "Little Better"] * 100) %>% in_text(inside = T)`, or "stayed the same" `r (cates_m1$cate[cates_m1$p == "Opposition" & cates_m1$resp == "Stayed Same"] * 100) %>% in_text()` where they took the political survey treatment. And, while they were `r (cates_m1$cate[cates_m1$p == "Opposition" & cates_m1$resp == "Lot Worse"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to say that the economy "got a lot worse", they were in fact `r (cates_m1$cate[cates_m1$p == "Opposition" & cates_m1$resp == "Little Worse"] * 100) %>% abs() %>% in_text(text = "percentage points", inside = F)` *less* likely to say that it "got a little worse". Evidence of any treatment effects here was only very weak. In both cases, the range of plausible values was large and included many values practically-equivalent to zero. Interestingly, non-voters showed a similar pattern of treatment effects to opposition voters, though were even more muted. This is perhaps unsurprising, given that the participants who comprised this group presumably had little sense of party identification.


# Political Surveys and Partisan Bias

One question remains unanswered: how much partisan bias do political surveys account for? With only a single experiment to draw upon, this is difficult to know. Yet we can approximate this proportion by assuming that my treatment effects represent upper-bounds on the true effect. As I discuss above, my estimates are likely conservative. As such, treating them as an *upper-* and not *lower-*bounds is also conservative as the true value may be larger.

Computing the proportion of bias *within* the experiment is simple if we rely on the log-odds changes shown in table \@ref(tab:tabA1). One need only divide the treatment's main effect and its interaction with partisanship by its main effect, its interaction, and the main effect of partisanship. In the present case, this suggests that around `r (((m1_posts$b_tTreatment + m1_posts$treatment_inc)/(m1_posts$b_tTreatment + m1_posts$treatment_inc + m1_posts$b_pInc))*100) %>% in_text(text = "percent", inside = F)` of the partisan bias present in incumbent supporters self-reported economic perceptions is due to the political survey context itself.

While informative, this estimate is limited only to a single case. It would be better to compute a *distribution* of proportions using data from many points in time. The British Election Study Internet Panel, 2014--2023 [@fieldhouse2020a], provides one such source of data. The BESIP includes the national economic perceptions item in fifteen separate waves. These cover the period between April 2014 and November 2019. I fit a similar ordered regression model to each wave of the data then, as the data do not vary the survey context, use the treatment effect from my survey experiment to approximate the proportion of partisan bias due to the political survey context under the assumption that it remains constant.

```{r bias-plot, fig.cap = 'Assuming that the estimates I present here represent upper-bounds, political surveys account for around one-quarter of all partisan bias in the economic perceptions that voters report in fifteen recent waves of the British Election Study Internet Panel (BESIP). In reality, these estimates are likely conservative. As such, the true amount of partisan bias that political surveys account for may be larger outside of election periods. Note that density plots here show the posterior distribution of the proportion of partisan bias attributable to the political survey context. Further, black bars show their 95% credible interval and point estimates their median.', fig.width = 6, fig.height = 7.5, echo = F}
bias_fig
```

Figure \@ref(fig:bias-plot) shows the resulting estimates. For incumbent supporters, these range from a low of `r (models$bias[models$wave == "wave 6" & models$var == "b_ionInc"]*100) %>% in_text(text = "percent", inside = F)` in wave 6 to a high of `r (models$bias[models$wave == "wave 13" & models$var == "b_ionInc"]*100) %>% in_text(text = "percent", inside = F)` in wave 13. The average across all waves is `r (models$bias[models$wave == "mean" & models$var == "b_ionInc"]*100) %>% in_text(text = "percent", inside = F)`, suggesting that around one-quarter of all partisan bias in the economic perceptions that incumbent partisans report is due to political survey effects. The equivalent effects for opposition supporters are much weaker and much more uncertain.


# Discussion and Conclusion

Survey research often proceeds as though survey respondents say what they mean. This is especially true when it comes to studying both the economic vote and voters' economic perceptions. Most often, this research assumes that differences between groups that exist *within the survey* reflect real differences that exist *outside of the survey* [@bullock2019]. My results show that this is not always the case. Some partisan bias arises simply due to the political survey context itself. In particular, I show that incumbent partisans report more positive economic perceptions in political compared to non-political surveys.

Why might this affect incumbent partisans but not opposition partisans? One explanation is that different partisans face different pressures when the economy is middling or poor. First, let us consider opposition partisans. When things are bad, these voters' primed and unprimed responses should coincide. Thus, they should show little difference in partisan bias. Now consider incumbent partisans, who face the opposite pressure. For this group, political priming leads them to report that things have gotten better. As a result, the economic climate causes a gap to open between the perceptions that they report in political and non-political survey contexts. If this is correct, and the state of the economy moderates political survey treatment effects, then future research might find that its effect reverses when the economy is doing well.

As the political survey context worsens partisan bias in self-reported economic perceptions, the most pressing issue is to work out how this affects applied research. For the economic vote, the outcome is mixed. Measuring economic perceptions in political surveys almost certainly worsens economic voting's endogeneity problem [@visconti2017; @evans2010; @evans2006]. Yet any effects appear limited only to incumbent partisans and much partisan bias remains constant. For political science in general, my results raise questions of measurement and validity. After all, these are likely not the only items sensitive to changes in survey context. Take items that measure attitudes towards immigration. It seems reasonable to expect voters to report different attitudes when primed to consider party politics or, say, changes in the labor market or even unrelated topics like dental hygiene.

The consequences are most serious for macro research. This is because small differences at the individual-level can yield large differences at the aggregate-level. Most often, this research aggregates these data in one of two ways. First, they compute the proportion of respondents who say that the economy has gotten better. However, my results suggest that doing so would over-estimate how rosy incumbent supporters think things really are where these figures are split by partisanship [see, for example, @enns2012a]. Second, they compute net economic perceptions. That is, the proportion of respondents who report that the economy has gotten better minus those who report that it has gotten worse. Yet using net figures both over-estimates incumbent positivity and under-estimates incumbent negativity. For example, the results in figure 2 suggest a difference in net economic perceptions of almost 12 percentage points depending on the survey context. Some might argue that this is no issue for economic voting research that most often relies on real economic statistics [though see @lewis-beck2013b]. This might be true. Even so, it would still be a problem for the broader analysis of attitudes and opinions in mass publics.

Future research should consider if political survey effects are constant or, instead, if they vary by external context. For example, I have alluded to the possibility that my results might change in good economic times. Ultimately, this remains to be tested. Nevertheless, my results suggest a fruitful avenue for future research: we might opt not to adjust our *models*, but rather to adjust our *designs*. Research in this vein has already begun [@visconti2017]. One obvious suggestion would be to field separate surveys to measure respondents' party political and non-party political attitudes and beliefs. Though this might be more costly, the gains could be considerable if it reduced nuisance variation that muddies our inferences. As a result, students of the economic vote might gain both a better understanding of how the economy affects voters' behavior and how voters come to update their economic perceptions.

\pagebreak


# References

<div id="refs"></div>


\pagebreak

\setcounter{table}{0}

\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}

\renewcommand{\thefigure}{A\arabic{figure}}


# Appendix A: Power Analysis

```{r power-plot, fig.cap = 'Outcomes from 6,000 simulation-based power analyses, ordered by lower 95% credible interval. In scenario 1, I assume that political survey effects account for the total effect I find in the observational data. In scenario 2, I assume instead that political survey effects account for half of this effect. All samples achieve 80% in scenario 1. Only a sample of 2,500 achieved 80% power in scenario 2.', fig.width = 6, fig.height = 3.75, echo = F}
power_fig
```

Before fielding my experiment, it was essential that I determine an appropriate sample size. To do so, I conducted a simulation-based power analysis. This approach was necessary as my dependent variable was ordinal. Unlike continuous data, it is not possible to conduct power analyses for ordinal data by hand. It was important that the effect sizes I used in my power analysis be of a realistic and reasonable size. To do so, I fit a similar ordered-probit model to the one I discussed above to data from wave 16 of the British Election Study Internet Panel [@fieldhouse2020a]. I then used the regression parameters to establish informative prior expectations for what effect sizes I might expect.

The BESIP data are observational. As such, the resulting estimates tell us only the net effect that respondents' past voting behavior and the political survey context have on the economic perceptions that they report. We do not, however, know what proportion of these effects each accounts for. I proposed two hypothetical scenarios as the basis of my power analysis. In the first, I assumed that political survey effects accounted for the total effect I observed in the observational data. In the second scenario, I instead assumed that they accounted for only half of it. I then simulated 1,000 experiments for each across three sample sizes ($n =$ 1,500, $n =$ 2,000, and $n =$ 2,500). These matched the blocked structure of my experiment. To make sure that my results did not depend on the random seed I used to simulate my data, I incremented it by one for each simulation. This gave 6,000 simulated experiments in total. In simulating my data, I focused on the treatment's effect on incumbent partisans. Further, I set my desired level of power at 80%. As I expect this effect to be positive, this implies that 80% of the effects from my simulation should have a lower 95% credible interval exceeds zero.

Figure \@ref(fig:power-plot) shows the outcomes of all 6,000 simulated experiments, ordered by their lower 95% credible interval. For scenario 1, all sample sizes achieved the desired level of power. Indeed, every simulated experiment yielded estimates that were greater than zero matter the sample size. This was not the case for scenario 2. Instead, every sample size included at least some simulations with lower 95% credible intervals that did not exceed zero. Here, a sample size of 1,500 corresponded with a power level of 60%; 2,000 with a power level of 74%; and 2,500 with a power level of 84%. Thus, I opted for the latter to exceed 80% power.

\pagebreak


# Appendix B: Questionnaire

\setlength\parindent{0em}

<!-- Column headers -->
\begin{minipage}[t]{0.4\linewidth}
\center{\sffamily{\textbf{Treatment}}}\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\center{\sffamily{\textbf{Control}}}\\
\end{minipage}

<!-- Question 1 -->
\begin{minipage}[t]{0.4\linewidth}
\sffamily \textbf{Q1.} \rmfamily If there were a general election held tomorrow, which party would you vote for?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Conservative}\\
\sffamily \textbf{2.} \rmfamily \emph{Labour}\\
\sffamily \textbf{3.} \rmfamily \emph{Liberal Democrat}\\
\sffamily \textbf{4.} \rmfamily \emph{Scottish National Party (SNP)}\\
\sffamily \textbf{5.} \rmfamily \emph{Plaid Cymru}\\
\sffamily \textbf{6.} \rmfamily \emph{Brexit Party}\\
\sffamily \textbf{7.} \rmfamily \emph{Green}\\
\sffamily \textbf{8.} \rmfamily \emph{Some other party}\\
\sffamily \textbf{9.} \rmfamily \emph{Would not vote}\\
\sffamily \textbf{10.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\sffamily \textbf{Q1.} \rmfamily Imagine that you need to buy toothpaste in the near future, which brand would you choose?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Colgate}\\
\sffamily \textbf{2.} \rmfamily \emph{Sensodyne}\\
\sffamily \textbf{3.} \rmfamily \emph{Aquafresh}\\
\sffamily \textbf{4.} \rmfamily \emph{Oral-B}\\
\sffamily \textbf{5.} \rmfamily \emph{Macleans}\\
\sffamily \textbf{6.} \rmfamily \emph{Arm \& Hammer}\\
\sffamily \textbf{7.} \rmfamily \emph{Crest}\\
\sffamily \textbf{8.} \rmfamily \emph{Some other brand}\\
\sffamily \textbf{9.} \rmfamily \emph{I would not buy toothpaste}\\
\sffamily \textbf{10.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}


<!-- Question 2 -->
\begin{minipage}[t]{0.4\linewidth}
\sffamily \textbf{Q2.} \rmfamily On a scale of 0 (certain NOT to vote) to 10 (absolutely certain to vote), how likely would you be to vote in a general election tomorrow?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{0 -- Certain NOT to vote}\\
\sffamily \textbf{2.} \rmfamily \emph{1}\\
\sffamily \textbf{3.} \rmfamily \emph{2}\\
\sffamily \textbf{4.} \rmfamily \emph{3}\\
\sffamily \textbf{5.} \rmfamily \emph{4}\\
\sffamily \textbf{6.} \rmfamily \emph{5}\\
\sffamily \textbf{7.} \rmfamily \emph{6}\\
\sffamily \textbf{8.} \rmfamily \emph{7}\\
\sffamily \textbf{9.} \rmfamily \emph{8}\\
\sffamily \textbf{10.} \rmfamily \emph{9}\\
\sffamily \textbf{11.} \rmfamily \emph{10 -- Absolutely certain to vote}\\
\sffamily \textbf{12.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\sffamily \textbf{Q2.} \rmfamily On a scale of 0 (not at all important) to 10 (very important), how important do you think dental hygiene is in everyday life?\\
\\
\\
\sffamily \textbf{1.} \rmfamily \emph{0 -- Not at all important}\\
\sffamily \textbf{2.} \rmfamily \emph{1}\\
\sffamily \textbf{3.} \rmfamily \emph{2}\\
\sffamily \textbf{4.} \rmfamily \emph{3}\\
\sffamily \textbf{5.} \rmfamily \emph{4}\\
\sffamily \textbf{6.} \rmfamily \emph{5}\\
\sffamily \textbf{7.} \rmfamily \emph{6}\\
\sffamily \textbf{8.} \rmfamily \emph{7}\\
\sffamily \textbf{9.} \rmfamily \emph{8}\\
\sffamily \textbf{10.} \rmfamily \emph{9}\\
\sffamily \textbf{11.} \rmfamily \emph{10 -- Very important}\\
\sffamily \textbf{12.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}


<!-- Question 3 -->
\begin{minipage}[t]{0.4\linewidth}
\sffamily \textbf{Q3.} \rmfamily Who do you think would make the best Prime Minister?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Boris Johnson}\\
\sffamily \textbf{2.} \rmfamily \emph{Jeremy Corbyn}\\
\sffamily \textbf{3.} \rmfamily \emph{Jo Swinson}\\
\sffamily \textbf{4.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\sffamily \textbf{Q3.} \rmfamily Generally speaking, what type of toothbrush do you use?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Manual}\\
\sffamily \textbf{2.} \rmfamily \emph{Electric}\\
\sffamily \textbf{3.} \rmfamily \emph{I do not have a toothbrush}\\
\sffamily \textbf{4.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}


<!-- Question 4 -->
\begin{minipage}[t]{0.4\linewidth}
\sffamily \textbf{Q4.} \rmfamily In hindsight, do you think Britain was right or wrong to vote to leave the European Union?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Right to leave}\\
\sffamily \textbf{2.} \rmfamily \emph{Wrong to leave}\\
\sffamily \textbf{3.} \rmfamily \emph{Neither right nor wrong}\\
\sffamily \textbf{4.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\sffamily \textbf{Q4.} \rmfamily When brushing your teeth, do you...\\
\\
\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Wet your toothbrush, then apply toothpaste?}\\
\sffamily \textbf{2.} \rmfamily \emph{Apply toothpaste, then wet your toothbrush?}\\
\sffamily \textbf{3.} \rmfamily \emph{Not wet your toothbrush at all}\\
\sffamily \textbf{4.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}


<!-- Question 5 -->
\begin{minipage}[t]{0.4\linewidth}
\sffamily \textbf{Q5.} \rmfamily How well or badly do you think the government are doing at handling Britain's exit from the European Union?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Very well}\\
\sffamily \textbf{2.} \rmfamily \emph{Fairly well}\\
\sffamily \textbf{3.} \rmfamily \emph{Neither well nor badly}\\
\sffamily \textbf{4.} \rmfamily \emph{Fairly badly}\\
\sffamily \textbf{5.} \rmfamily \emph{Very badly}\\
\sffamily \textbf{6.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\linewidth}
\sffamily \textbf{Q5.} \rmfamily Generally speaking, on average how many times do you brush your teeth every day?\\
\\
\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Never}\\
\sffamily \textbf{2.} \rmfamily \emph{Once}\\
\sffamily \textbf{3.} \rmfamily \emph{Twice}\\
\sffamily \textbf{4.} \rmfamily \emph{Three times}\\
\sffamily \textbf{5.} \rmfamily \emph{More than three times}\\
\sffamily \textbf{6.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}

\pagebreak

<!-- Treatment ends-->
\begin{minipage}[t]{\linewidth}
\sffamily{\textbf{Treatment ends. Subsequent questions are identical for each group.}}\\
\end{minipage}


<!-- Question 6 -- All -->
\begin{minipage}[t]{\linewidth}
\sffamily \textbf{Q6.} \rmfamily Now, a few questions about economic conditions. How does the \emph{financial situation of your household} now compare with what it was 12 months ago?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Got a lot worse}\\
\sffamily \textbf{2.} \rmfamily \emph{Got a little worse}\\
\sffamily \textbf{3.} \rmfamily \emph{Stayed the same}\\
\sffamily \textbf{4.} \rmfamily \emph{Got a little better}\\
\sffamily \textbf{5.} \rmfamily \emph{Got a lot better}\\
\sffamily \textbf{6.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}


<!-- Question 7 -- All -->
\begin{minipage}[t]{\linewidth}
\sffamily \textbf{Q7.} \rmfamily How do you think the \emph{general economic situation} in this country has changed over the last 12 months?\\
\\
\sffamily \textbf{1.} \rmfamily \emph{Got a lot worse}\\
\sffamily \textbf{2.} \rmfamily \emph{Got a little worse}\\
\sffamily \textbf{3.} \rmfamily \emph{Stayed the same}\\
\sffamily \textbf{4.} \rmfamily \emph{Got a little better}\\
\sffamily \textbf{5.} \rmfamily \emph{Got a lot better}\\
\sffamily \textbf{6.} \rmfamily \emph{Don't know}\\
\\
\end{minipage}

\setlength\parindent{2em}

\pagebreak


# Appendix C: Prior Distributions for Ordered Regression Models

As I discuss in my methods section above, my experiment includes an outcome variables that is ordinal rather than continuous or binary. Though others often treat these data as though they are continuous for the sake of convenience, this practice is prone to a whole host of serious inferential pitfalls. Further, though more robust, almost all conventional ordered regression models face similar issues. As such, I use Bayesian methods to implement an extended ordered regression model that overcomes these problems, thereby allowing me to estimate any treatment effects in a principled manner that respects the nature of the data.

Though similar, the Bayesian approach to statistical analysis does introduce some points of difference compared to the classical statistics that dominates much political science research. Most notably, it requires that one specify a prior distribution over each parameter in one's model before fitting it to the data^[For an introduction to Bayesian statistics and Bayesian methods, see @mcelreath2020; @lambert2018; @kruschke2017; @kruschke2015]. As well as allowing us to shift focus from the likelihood (*"what is the probability of the data given the hypothesis?"*) to the posterior distribution (*"what is the probability of the hypothesis given the data?"*), these "priors" also serve two useful purposes. First, they allow us to incorporate any pre-existing knowledge that we might be privy to into our models. This might include the results of a previous analysis (thereby having our model expect results similar to the previous case before it sees the data) or simply our understanding of the nature of the model and what values it is reasonable for certain parameters to take (for example, we know that it is not possible for probabilities to be negative). Second, they make our models skeptical by nature and shrink any parameter estimates towards the prior. This "regularization" protects against over-fitting common to maximum likelihood-based approaches, especially where sample sizes are small [@mcelreath2020; @lambert2018; @gelman2014a].

In this case, there is little existing evidence on which to draw. Though others have fielded the same economic perception items in the past, they have tended to do so in an observational, rather than an experimental, context. As a result, I do not have a good idea about what values my parameters might take before I fit the model to the data. To complicate matters further, ordered regression models have many moving parts, which interact with one another to produce the implied ordinal outcome. To overcome this complication, I take a principled approach below and decide my priors based on a series of prior predictive simulations. In doing so, I seek to ensure that all my decisions be conservative so that any treatment effects I estimate will be robust. This has two implications. First, that I should assume all effects to be zero before fitting my model to the data. Second, that I should also assume all possible combination of response probabilities to be equally likely. The various ordered regression models that I present in this paper rely on the same three sets of parameters, each of which serves a distinct function. I discuss each specific parameter in turn, below.


## Threshold Parameters

Ordered regression models work by translating between an ordinal variable that we observe and a continuous variable that we do not. To perform this feat, they rely on a series of threshold parameters that split the latent continuous distribution into as many segments as their are response options. The are of each segment then corresponds to the probability that the response option that it represents will occur. Absent any knowledge about the nature of the data, the most conservative assumption that we can make is that any possible combination of responses is as likely as any other.

As we measure each response option in terms of its probability of occurring, it is worth also thinking on the probability scale when setting our priors. Thus, given this assumption, we should expect the probability of a threshold parameter landing on any point on the probability scale to be constant. The issue then is to find a prior on the latent probit scale on which the model operates, which ranges from $-\infty$ to $+\infty$, that gives a flat prior on the probability scale that we really care about, which is bounded by 0 and 1. Fortunately, this is not difficult and the answer is relatively well-known: a Normal(0, 1) prior on the former gives a uniform prior on the latter.

```{r thresh-plot, fig.cap = 'Before seeing the data, the most conservative assumption that we can make about the distribution of response options is that each combination is as likely as any other. This implies that prior thresholds must be able to take any possible value on the probability scale, conditional on the constraints of the model itself. A Normal(0, 1) prior on the latent probit scale yields fulfills this requirement and implies a flat prior on the probability scale. The figure above shows the resulting prior threshold parameters. Note also that the probability of a threshold occurring at any point on the scale is constant over the entire range of the scale (with any deviations arising only due to random noise in the simulation process).', fig.width = 6, fig.height = 3.25, echo = F}
thresh_fig
```

The four left-most panels of figure \@ref(fig:thresh-plot) show the resulting prior distributions that this collective prior over all thresholds implies for each specific threshold. The right-most panel instead shows the implied prior over the whole probability scale (i.e. the distribution that we would find were we to stack each threshold distribution on top of each other). As the histograms in the figure make clear, the implied priors for each threshold are non-informative and, in all cases, take a wide range of possible values. For example, the priors shown here allow for a non-zero probability that the first threshold occurs as high as the $80\%$ mark and the fourth threshold as low as $20\%$. That the first response option corresponds to respondents reporting that the economy "got a lot worse" and the last that it has "got a lot better" reaffirms just how non-informative these priors really are.

Though we do not specify priors for each specific threshold, these prior predictive simulations suggest that they take their own distinctive shapes nonetheless. This phenomenon arises due to the constraints that both the prior and the model impose on the values that these parameters can take. For example, each threshold is constrained to take only values smaller than those of the threshold that follow it. As a result, it is not possible for any threshold to cover the entire space as this would leave the others with nowhere to go. Likewise, the collective nature of the prior means that the priors for each threshold *must* result in a flat prior overall. The result is the set of symmetrical distributions that we see above.


## Beta Parameters

Threshold parameters segment the latent outcome distribution, though do not move. Beta parameters, instead, shift the latent distribution up and down its scale. This movement then serves to shift the probability mass of each observed response option in turn. As such, we can interpret beta parameters much like regression coefficients in linear and logistic regression models, which perform a similar role.

Before seeing the data, the most conservative assumption that one can make about these effects is that they are equal to zero (i.e. that they are null). Doing so is simple and, in the absence of any better information, uncontroversial. More difficult however is determining how uncertain these priors should be. One the one hand, a tight prior around zero will be very conservative, but perhaps to the extent that it ignores perfectly informative data. On the other, a loose prior will pay closer attention to the data, but perhaps to the extent that it will result in over-fitting.

For models with continuous outcomes, things are straightforward. If the prior is very wide, then it is also likely to cover the full range of plausible values that its respective parameter might take. But ordinal variables are not continuous and, as a result, this common practice can lead to perverse implications. Unlike models with continuous outcomes, wide priors on the latent probit scale *do not* give wide priors on the outcome scale. This is because the outcome scale takes only a finite set of discrete values. As a result, wide priors on the latent probit scale instead imply U-shaped priors due to probability mass piling up at the extremes. This problem then multiplies --- quite literally --- where the data include variables that exhibit a high degree of variation (for example age, which in the study of voting behavior might take any value between 18 and 100) or where the sum of all variables is large (such as when a model contains many parameters).

```{r beta-plot, fig.cap = 'While it is common to set wide priors on beta values where the outcome is continuous, such "non-informative" priors have perverse consequences when the outcome is ordered. This is because they make the latent scale too diffuse, thereby concentrating almost all of the prior probability mass at the two extremes of the observed ordinal outcome scale. Further, models that include many independent variables or independent variables that take extreme values worsen this problem further.', fig.width = 6, fig.height = 3.25, echo = F}
beta_fig
```

Figure \@ref(fig:beta-plot) displays this phenomenon across different priors and different values of beta. Where these sum to zero, only the thresholds determine the response distribution, which I fix to ensure that each response has a prior probability of $20\%$ where betas sum to zero. As the figure shows, when this sum exceeds zero the prior probability of responding with either a 1 or a 5 increases. This is true for all priors, though the effect is most pronounced where the prior standard deviations are large. In each model in this paper, the sum of parameters increases where respondents voted at the last election or are in the treatment group. In light of this, using a prior on beta with a large standard deviation is akin to assuming that these participants are more likely to say either that the economy has "got a lot worse" or "got a lot better". Perhaps counter-intuitively, smaller standard deviations are, thus, less informative. Thus, I use the least informative prior --- Normal(0, 0.25) --- for all beta values in my models.


## Delta Parameters

```{r delta-plot, fig.cap = 'Setting diffuse priors on delta parameters can also have perverse consequences. In this case, they instead concentrate the prior probability mass in the middle and at the two extremes of the observed ordinal variable. Again, this is likely to be worse where models also include many independent variables or independent variables that take extreme values.', fig.width = 6, fig.height = 3.25, echo = F}
delta_fig
```

Whereas beta parameters shift the latent outcome distribution, delta parameters instead compress or disperse it at a given point. This serves to redistribute the observe outcome's probability mass towards central or extreme responses, conditional on its place on the scale. As in the previous case, the most conservative assumption that we can make before seeing the data is to expect these parameters to be equal to zero. Where this is true, the standard deviation of the latent outcome distribution does not vary across participants. Again, this is simple to achieve and, again, things become more complicated when it comes to setting the standard deviation. The problem is the same as before: large standard deviations imply more, not less, informative outcomes. 

Figure \@ref(fig:delta-plot) is similar to figure A4 and shows the implication that different priors and different values of delta have on the implied prior outcome distribution. As before, I fix all thresholds in this case to imply an equal chance of any response option being selected and also fix all beta parameters to 0. While wide priors on the beta parameters produced U-shaped distributions, wide priors on the delta parameters do not. Instead, they increase the prior probability of central and extreme responses, resulting in a crown-like distribution. Note, however, that this pattern is conditional on the choice of thresholds and that U-shaped distributions may arise here too under different circumstances. As before, each response has an equal probability where the sum of delta parameters is zero. As this sum increases, the central response option becomes much more likely and extreme responses somewhat more likely. This implies that tighter standard deviations are also less informative in this case too. Given this, I opt to use 

While the beta parameters produced U-shaped distributions, these do not. Instead, they increase the prior probability of central and extreme responses. Note, however, that this is conditional on the choice of thresholds. Where these do not equalize the probability of each response, a U-shape can still emerge. As before, each response has an equal probability when the sum of parameter values is zero. As this sum increases, the central response becomes much more likely. Likewise, extreme responses become somewhat more likely too. Given this, I take the right-most distribution --- a Normal(0, 0.25) prior --- as the prior for each of my delta values as it remains most constant across all cases.

\pagebreak


# Appendix D: Robustness Checks

There are three plausible objections to the results I report above. First, that the treatment effects occur due to some mechanism other than partisan bias. Second, that the theory does not generalize to other types of electoral identification. And, third, that the results are sensitive to my model specification. I test each below. The first tests if the treatment mechanism relies on partisan bias. To do so, I apply the same test to participants' reported *personal* economic perceptions. Past research finds that these show little sensitivity to party identification. The second tests if the theory generalizes to other types of identification. In particular, voting behavior at the 2016 referendum on European Union membership. The third tests if the findings are robust to different methods. In this case, by substituting ordered regression for multinomial regression instead.


## Personal Economic Perceptions and Partisan Bias as a Potential Mechanism

```{r pers-plot, fig.cap = 'Political surveys do not cause voters to report different perceptions of their own personal finances (left panel). This is unsurprising, since prior research shows that they are much less sensitive to party identification. Positive values imply that those in the treatment group were more likely to report a given response. Negative values imply the opposite. In general, treatment effects showed the expected signs. Incumbent voters were more positive. Likewise, opposition voters were more negative. Even so, in all cases, the distribution of treatment estimates were centered on small values and had a plausible chance of being practically-equivalent to zero. Here, density plots show the posterior distribution of conditional average treatment effects. Further, black bars show their 95% credible intervals and point estimates their medians.', fig.width = 6, fig.height = 3.25, echo = F}
per_fig
```

Above, I assume that my findings result from partisan bias. This seems reasonable given existing research [@devries2018; @bartels2002; @conover1987]. Even so, a skeptic might argue that I have not yet provided good evidence that this is indeed the case. Instead, they might argue that some other mechanism is reasonable for my findings. As a result, the pattern that I observe might also apply to any other dependent variable. This is a reasonable objection, as my design does not allow me to tease apart any intermediary steps in the causal chain between survey context and reported economic perceptions. Fortunately, there are ways to reduce this uncertainty. One is to test how the treatment affects a similar item that we know suffers from little partisan bias. Voters' perceptions of their own personal finances are on such possibility. Like national-level items, these too have their origin in consumer confidence surveys [@katona1951]. But, unlike national-level items, they are much less sensitive to partisan bias. This makes sense. After all, many would argue that the government is less accountable for any one person's well-being than it is for the well-being of the nation as a whole [@lewis-beck2017; @lewis-beck2000; @paldam1981; @kinder1981; @kinder1979; though see @tilley2018].

Figure \@ref(fig:pers-plot) shows how the treatment affected the personal economic perceptions that my participants reported. As before, I condition these estimates on prior voting behavior for the same reasons as above. In this case, all treatment effects have the expected signs. That is, incumbent supporters are more positive and opposition supporters more negative under the treatment. This might, then, suggest the presence of at least some partisan bias. Yet, in all cases, point estimates are small. These range in size from only `r (cates_m2$cate[cates_m2$p == "Opposition" & cates_m2$resp == "Lot Better"] * 100) %>% in_text(inside = F)` to `r (cates_m2$cate[cates_m2$p == "Incumbent" & cates_m2$resp == "Little Better"] * 100) %>% in_text(text = "percentage points", inside = F)`. Further, these effects have 95% credible intervals that, in all cases, are very uncertain.

Taken together, these results suggest little evidence that political surveys affect the personal economic perceptions that respondents report. Were some other mechanism responsible for the treatment effects I find, this might not be the case. Instead, I find that the treatment might have a similar effect for both items. Instead, both sets of results are consistent with existing theory and the argument that I present above. That is, respondents must have a reason to assign responsibility to the government if political surveys are to prime respondents to respond in a different way. This does not seem to be the case for perceptions of one's personal finances. Instead, they appear to exhibit little partisan bias, leaving the treatment with nothing to manipulate. Of course, it is never possible to rule out any other mechanism with absolute certainty. Still, these results do at least make such a possibility seem much less likely.


## Generalization of Treatment Effects Across Different Types of Electoral Identification

If the theory that underpins my analysis is robust, it should generalize to other types of political identification. The British case is useful in this respect. Due to the 2016 referendum on EU membership. the country now has *two* forms of electoral identification^[And yet more still in Scotland, where unionist versus nationalist identities rose to prominence as a result of the 2014 referendum on Scottish independence.]. The first is conventional party identification. The second is identification with either the Leave or Remain side at the EU referendum. Further, recent evidence shows that the latter also affects how voters report to perceive the economy [@fieldhouse2020; @sorace2018]. As the Leave side won, supporting it is now, for all intents and purposes, akin to supporting the incumbent party. By the same logic, supporting Remain is now akin to supporting an opposition party. Accordingly, we should expect any treatment effects to generalize to EU referendum identification in the same way as does party identification.

\input{_assets/tab1.tex}

```{r eu-plot, fig.cap = 'Political surveys cause participants to report different perceptions of the national economy, conditional on their voting behavior at the 2016 referendum on European Union membership. Like with party identification, these effects are most pronounced where they voted for the winning side (Leave). But, in this case, there is also good evidence of a treatment effect on Remain voters too. Positive values imply that those in the treatment group were more likely to report a given response. Negative values imply the opposite. Density plots show the posterior distribution of conditional average treatment effects. Black bars show their 95% credible intervals and point estimates their medians.', fig.width = 6, fig.height = 3.25, echo = F}
eu_fig
```

EU and party identification are not unrelated. But, the former does still cut across the latter to a meaningful extent. Table \@ref(tab:tab1) makes this clear. It shows the proportion of participants who voted for each combination of options at the 2016 referendum on EU membership and the 2017 general election. As we can see, participants who voted for the incumbent Conservative Party in 2017 most often voted to leave in 2016. Likewise, those who voted for an opposition party most often voted to remain. But this is not true in all cases. For example, 27.3% of participants who voted for the incumbent Conservative Party also voted to remain in the EU. Similarly, 26.4% of participants who voted for an opposition party also voted to leave. Further, 12.1% of participants voted only in 2016. Thus, we should not expect treatment effects for EU identification to be mere reflections of those across party identification.

Fortunately, the fourth question on the political survey primed voters to consider how they voted at the 2016 referendum (see appendix B). Figure \@ref(fig:eu-plot) shows the corresponding treatment effects. In this case, Leave supporters in the treatment group were `r (cates_m3$cate[cates_m3$r == "Leave" & cates_m3$resp == "Little Worse"] * 100) %>% in_text(text = "percentage points", inside = F)` less likely to report either that the economy "got a little worse" and `r (cates_m3$cate[cates_m3$r == "Leave" & cates_m3$resp == "Lot Worse"] * 100) %>% in_text(text = "percentage points", inside = F)` less likely to say that it "got a lot worse". They were also `r (cates_m3$cate[cates_m3$r == "Leave" & cates_m3$resp == "Stayed Same"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to report that it had "stayed the same" and `r (cates_m3$cate[cates_m3$r == "Leave" & cates_m3$resp == "Little Better"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to report that it "got a little better". Again, almost no one said that the economy "got a lot better" and there was no meaningful treatment effect `r (cates_m3$cate[cates_m3$r == "Leave" & cates_m3$resp == "Lot Better"] * 100) %>% in_text()`.

Those who voted Remain also showed similar effects to opposition voters. Yet they were much more likely to say that the economy "got a lot worse" in the last twelve months. This effect was large `r (cates_m3$cate[cates_m3$r == "Remain" & cates_m3$resp == "Lot Worse"] * 100) %>% in_text()`. Further, though its 95% credible interval crossed zero, `r (length(cates_m3$cate[cates_m3$r == "Remain" & cates_m3$resp == "Lot Worse" & cates_m3$cate > 0])/length(cates_m3$cate[cates_m3$r == "Remain" & cates_m3$resp == "Lot Worse"])) %>% scales::percent()` of the posterior distribution was greater than zero. Thus, we can be reasonably confident that the true effect is, in fact, greater than zero. Likewise, given these results, we can also be confident that the treatment generalizes to other types of electoral identification too.


## Sensitivity of Treatment Effects to Modeling Assumptions

Ordered regression models estimate effects that are consistent across threshold parameters and, thus, across responses. This is known as the proportional odds assumption [@agresti2010; @mccullagh1980]. Consider the present case. The treatment has a positive effect on the national economic perceptions that incumbents report when measured on the probit scale (see Table A2 in the appendix). This is why they are more likely to say that the economy "got a little better" or "stayed the same" and less likely to say that the economy "got a little worse" or "got a lot worse". But, of course, this assumption may not hold. Instead, the treatment might have a unique effect on each response option.

```{r multi-plot, fig.cap = 'Using a multinomial rather than an ordinal model does little to change the results. We still find that political surveys cause incumbent voters to report more positive economic perceptions (left panel). Positive values imply that those in the treatment group were more likely to report a given response. Negative values imply the opposite. Density plots show the posterior distribution of conditional average treatment effects. Further, black bars show their 95% credible intervals and point estimates their medians.', fig.width = 6, fig.height = 3.25, echo = F}
multi_fig
```

To relax this assumption, we can use multinomial regression instead. Figure A7 shows the resulting estimates from such a model. Note that the multinomial model is less efficient and, thus, estimates tend to be less precise. Even so, they still lead to the same conclusion: that political survey effects are most clear where participants voted for the incumbent at the last election. Here, incumbent voters were `r (cates_m4$cate[cates_m4$p == "Incumbent" & cates_m4$resp == "Little Worse"] * 100) %>% in_text(text = "percentage points", inside = F)` less likely to say that the economy "got a little worse" and `r (cates_m4$cate[cates_m4$p == "Incumbent" & cates_m4$resp == "Lot Worse"] * 100) %>% in_text(text = "percentage points", inside = F)` that it "got a lot worse". They were also `r (cates_m4$cate[cates_m4$p == "Incumbent" & cates_m4$resp == "Stayed Same"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to say that the economy had "stayed the same" and `r (cates_m4$cate[cates_m4$p == "Incumbent" & cates_m4$resp == "Little Better"] * 100) %>% in_text(text = "percentage points", inside = F)` more likely to say that it "got a little better". Results for opposition supporters and non-voters differ little to the results in figure \@ref(fig:nat-plot). Further, there appear to be no difference in the propensity of respondents to answer "Don't know" under the political survey treatment compared to the non-political survey control. As such, my conclusions appear robust to both model specification and missing data.

\pagebreak

\input{_assets/tabA1.tex}

\pagebreak

\input{_assets/tabA2.tex}

\pagebreak

\input{_assets/tabA3.tex}

\pagebreak

\input{_assets/tabA4.tex}
